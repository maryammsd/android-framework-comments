{
  "filePath" : "/home/maryam/clearblue/files/android-source-35/android/adservices/ondevicepersonalization/InferenceInput.java",
  "packageName" : "android.adservices.ondevicepersonalization",
  "className" : "InferenceInput",
  "comment" : "\n * Contains all the information needed for a run of model inference. The input of {@link\n * ModelManager#run}.\n ",
  "links" : [ "android.adservices.ondevicepersonalization.ModelManager#run" ],
  "variables" : [ {
    "name" : "mParams",
    "type" : "Params",
    "comment" : " The configuration that controls runtime interpreter behavior. ",
    "links" : [ ]
  }, {
    "name" : "mInputData",
    "type" : "Object[]",
    "comment" : "\n     * An array of input data. The inputs should be in the same order as inputs of the model.\n     *\n     * <p>For example, if a model takes multiple inputs:\n     *\n     * <pre>{@code\n     * String[] input0 = {\"foo\", \"bar\"}; // string tensor shape is [2].\n     * int[] input1 = new int[]{3, 2, 1}; // int tensor shape is [3].\n     * Object[] inputData = {input0, input1, ...};\n     * }</pre>\n     *\n     * For TFLite, this field is mapped to inputs of runForMultipleInputsOutputs:\n     * https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/InterpreterApi#parameters_9\n     ",
    "links" : [ ]
  }, {
    "name" : "mBatchSize",
    "type" : "int",
    "comment" : "\n     * The number of input examples. Adopter can set this field to run batching inference. The batch\n     * size is 1 by default. The batch size should match the input data size.\n     ",
    "links" : [ ]
  }, {
    "name" : "mExpectedOutputStructure",
    "type" : "InferenceOutput",
    "comment" : "\n     * The empty InferenceOutput representing the expected output structure. For TFLite, the\n     * inference code will verify whether this expected output structure matches model output\n     * signature.\n     *\n     * <p>If a model produce string tensors:\n     *\n     * <pre>{@code\n     * String[] output = new String[3][2];  // Output tensor shape is [3, 2].\n     * HashMap<Integer, Object> outputs = new HashMap<>();\n     * outputs.put(0, output);\n     * expectedOutputStructure = new InferenceOutput.Builder().setDataOutputs(outputs).build();\n     * }</pre>\n     ",
    "links" : [ ]
  } ],
  "methods" : [ {
    "name" : "public Params getParams()",
    "returnType" : "Params",
    "comment" : " The configuration that controls runtime interpreter behavior. ",
    "links" : [ ]
  }, {
    "name" : "public Object[] getInputData()",
    "returnType" : "Object[]",
    "comment" : "\n     * An array of input data. The inputs should be in the same order as inputs of the model.\n     *\n     * <p>For example, if a model takes multiple inputs:\n     *\n     * <pre>{@code\n     * String[] input0 = {\"foo\", \"bar\"}; // string tensor shape is [2].\n     * int[] input1 = new int[]{3, 2, 1}; // int tensor shape is [3].\n     * Object[] inputData = {input0, input1, ...};\n     * }</pre>\n     *\n     * For TFLite, this field is mapped to inputs of runForMultipleInputsOutputs:\n     * https://www.tensorflow.org/lite/api_docs/java/org/tensorflow/lite/InterpreterApi#parameters_9\n     ",
    "links" : [ ]
  }, {
    "name" : "public int getBatchSize()",
    "returnType" : "int",
    "comment" : "\n     * The number of input examples. Adopter can set this field to run batching inference. The batch\n     * size is 1 by default. The batch size should match the input data size.\n     ",
    "links" : [ ]
  }, {
    "name" : "public InferenceOutput getExpectedOutputStructure()",
    "returnType" : "InferenceOutput",
    "comment" : "\n     * The empty InferenceOutput representing the expected output structure. For TFLite, the\n     * inference code will verify whether this expected output structure matches model output\n     * signature.\n     *\n     * <p>If a model produce string tensors:\n     *\n     * <pre>{@code\n     * String[] output = new String[3][2];  // Output tensor shape is [3, 2].\n     * HashMap<Integer, Object> outputs = new HashMap<>();\n     * outputs.put(0, output);\n     * expectedOutputStructure = new InferenceOutput.Builder().setDataOutputs(outputs).build();\n     * }</pre>\n     ",
    "links" : [ ]
  }, {
    "name" : "public boolean equals(@android.annotation.Nullable Object o)",
    "returnType" : "boolean",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "public int hashCode()",
    "returnType" : "int",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "private void __metadata()",
    "returnType" : "void",
    "comment" : "",
    "links" : [ ]
  } ],
  "methodNames" : [ "public Params getParams()", "public Object[] getInputData()", "public int getBatchSize()", "public InferenceOutput getExpectedOutputStructure()", "public boolean equals(@android.annotation.Nullable Object o)", "public int hashCode()", "private void __metadata()" ],
  "variableNames" : [ "mParams", "mInputData", "mBatchSize", "mExpectedOutputStructure" ]
}