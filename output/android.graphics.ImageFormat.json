{
  "filePath" : "/home/maryam/clearblue/files/android-source-30/android/graphics/ImageFormat.java",
  "packageName" : "android.graphics",
  "className" : "ImageFormat",
  "comment" : "",
  "variables" : [ {
    "name" : "UNKNOWN",
    "type" : "int",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "RGB_565",
    "type" : "int",
    "comment" : "\n     * RGB format used for pictures encoded as RGB_565. See\n     * {@link android.hardware.Camera.Parameters#setPictureFormat(int)}.\n     ",
    "links" : [ "android.hardware.Camera.Parameters#setPictureFormat(int)" ]
  }, {
    "name" : "YV12",
    "type" : "int",
    "comment" : "\n     * <p>Android YUV format.</p>\n     *\n     * <p>This format is exposed to software decoders and applications.</p>\n     *\n     * <p>YV12 is a 4:2:0 YCrCb planar format comprised of a WxH Y plane followed\n     * by (W/2) x (H/2) Cr and Cb planes.</p>\n     *\n     * <p>This format assumes\n     * <ul>\n     * <li>an even width</li>\n     * <li>an even height</li>\n     * <li>a horizontal stride multiple of 16 pixels</li>\n     * <li>a vertical stride equal to the height</li>\n     * </ul>\n     * </p>\n     *\n     * <pre> y_size = stride * height\n     * c_stride = ALIGN(stride/2, 16)\n     * c_size = c_stride * height/2\n     * size = y_size + c_size * 2\n     * cr_offset = y_size\n     * cb_offset = y_size + c_size</pre>\n     *\n     * <p>For the {@link android.hardware.camera2} API, the {@link #YUV_420_888} format is\n     * recommended for YUV output instead.</p>\n     *\n     * <p>For the older camera API, this format is guaranteed to be supported for\n     * {@link android.hardware.Camera} preview images since API level 12; for earlier API versions,\n     * check {@link android.hardware.Camera.Parameters#getSupportedPreviewFormats()}.\n     *\n     * <p>Note that for camera preview callback use (see\n     * {@link android.hardware.Camera#setPreviewCallback}), the\n     * <var>stride</var> value is the smallest possible; that is, it is equal\n     * to:\n     *\n     * <pre>stride = ALIGN(width, 16)</pre>\n     *\n     * @see android.hardware.Camera.Parameters#setPreviewCallback\n     * @see android.hardware.Camera.Parameters#setPreviewFormat\n     * @see android.hardware.Camera.Parameters#getSupportedPreviewFormats\n     * </p>\n     ",
    "links" : [ "android.hardware.Camera#setPreviewCallback", "#YUV_420_888", "android.hardware.Camera.Parameters#getSupportedPreviewFormats()", "android.hardware.camera2", "android.hardware.Camera" ]
  }, {
    "name" : "Y8",
    "type" : "int",
    "comment" : "\n     * <p>Android Y8 format.</p>\n     *\n     * <p>Y8 is a YUV planar format comprised of a WxH Y plane only, with each pixel\n     * being represented by 8 bits. It is equivalent to just the Y plane from {@link #YV12}\n     * format.</p>\n     *\n     * <p>This format assumes\n     * <ul>\n     * <li>an even width</li>\n     * <li>an even height</li>\n     * <li>a horizontal stride multiple of 16 pixels</li>\n     * </ul>\n     * </p>\n     *\n     * <pre> size = stride * height </pre>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.hardware.camera2.CameraDevice} (if\n     * supported) through a {@link android.media.ImageReader} object. The\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} will return a\n     * single plane containing the pixel data. The pixel stride is always 1 in\n     * {@link android.media.Image.Plane#getPixelStride()}, and the\n     * {@link android.media.Image.Plane#getRowStride()} describes the vertical\n     * neighboring pixel distance (in bytes) between adjacent rows.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.ImageReader\n     * @see android.hardware.camera2.CameraDevice\n     ",
    "links" : [ "android.media.Image.Plane#getRowStride()", "android.media.Image#getPlanes()", "android.media.Image", "#YV12", "android.media.Image.Plane#getPixelStride()", "android.hardware.camera2.CameraDevice", "android.media.ImageReader" ]
  }, {
    "name" : "Y16",
    "type" : "int",
    "comment" : "\n     * <p>Android Y16 format.</p>\n     *\n     * Y16 is a YUV planar format comprised of a WxH Y plane, with each pixel\n     * being represented by 16 bits. It is just like {@link #Y8}, but has 16\n     * bits per pixel (little endian).</p>\n     *\n     * <p>This format assumes\n     * <ul>\n     * <li>an even width</li>\n     * <li>an even height</li>\n     * <li>a horizontal stride multiple of 16 pixels</li>\n     * </ul>\n     * </p>\n     *\n     * <pre> y_size = stride * height </pre>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.hardware.camera2.CameraDevice}\n     * through a {@link android.media.ImageReader} object if this format is\n     * supported by {@link android.hardware.camera2.CameraDevice}.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.ImageReader\n     * @see android.hardware.camera2.CameraDevice\n     *\n     * @hide\n     ",
    "links" : [ "#Y8", "android.media.Image", "android.hardware.camera2.CameraDevice", "android.media.ImageReader" ]
  }, {
    "name" : "NV16",
    "type" : "int",
    "comment" : "\n     * YCbCr format, used for video.\n     *\n     * <p>For the {@link android.hardware.camera2} API, the {@link #YUV_420_888} format is\n     * recommended for YUV output instead.</p>\n     *\n     * <p>Whether this format is supported by the old camera API can be determined by\n     * {@link android.hardware.Camera.Parameters#getSupportedPreviewFormats()}.</p>\n     *\n     ",
    "links" : [ "#YUV_420_888", "android.hardware.Camera.Parameters#getSupportedPreviewFormats()", "android.hardware.camera2" ]
  }, {
    "name" : "NV21",
    "type" : "int",
    "comment" : "\n     * YCrCb format used for images, which uses the NV21 encoding format.\n     *\n     * <p>This is the default format\n     * for {@link android.hardware.Camera} preview images, when not otherwise set with\n     * {@link android.hardware.Camera.Parameters#setPreviewFormat(int)}.</p>\n     *\n     * <p>For the {@link android.hardware.camera2} API, the {@link #YUV_420_888} format is\n     * recommended for YUV output instead.</p>\n     ",
    "links" : [ "#YUV_420_888", "android.hardware.Camera.Parameters#setPreviewFormat(int)", "android.hardware.camera2", "android.hardware.Camera" ]
  }, {
    "name" : "YUY2",
    "type" : "int",
    "comment" : "\n     * YCbCr format used for images, which uses YUYV (YUY2) encoding format.\n     *\n     * <p>For the {@link android.hardware.camera2} API, the {@link #YUV_420_888} format is\n     * recommended for YUV output instead.</p>\n     *\n     * <p>This is an alternative format for {@link android.hardware.Camera} preview images. Whether\n     * this format is supported by the camera hardware can be determined by\n     * {@link android.hardware.Camera.Parameters#getSupportedPreviewFormats()}.</p>\n     ",
    "links" : [ "#YUV_420_888", "android.hardware.Camera.Parameters#getSupportedPreviewFormats()", "android.hardware.camera2", "android.hardware.Camera" ]
  }, {
    "name" : "JPEG",
    "type" : "int",
    "comment" : "\n     * Compressed JPEG format.\n     *\n     * <p>This format is always supported as an output format for the\n     * {@link android.hardware.camera2} API, and as a picture format for the older\n     * {@link android.hardware.Camera} API</p>\n     ",
    "links" : [ "android.hardware.camera2", "android.hardware.Camera" ]
  }, {
    "name" : "DEPTH_JPEG",
    "type" : "int",
    "comment" : "\n     * Depth augmented compressed JPEG format.\n     *\n     * <p>JPEG compressed main image along with XMP embedded depth metadata\n     * following ISO 16684-1:2011(E).</p>\n     ",
    "links" : [ ]
  }, {
    "name" : "YUV_420_888",
    "type" : "int",
    "comment" : "\n     * <p>Multi-plane Android YUV 420 format</p>\n     *\n     * <p>This format is a generic YCbCr format, capable of describing any 4:2:0\n     * chroma-subsampled planar or semiplanar buffer (but not fully interleaved),\n     * with 8 bits per color sample.</p>\n     *\n     * <p>Images in this format are always represented by three separate buffers\n     * of data, one for each color plane. Additional information always\n     * accompanies the buffers, describing the row stride and the pixel stride\n     * for each plane.</p>\n     *\n     * <p>The order of planes in the array returned by\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} is guaranteed such that\n     * plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V (Cr).</p>\n     *\n     * <p>The Y-plane is guaranteed not to be interleaved with the U/V planes\n     * (in particular, pixel stride is always 1 in\n     * {@link android.media.Image.Plane#getPixelStride() yPlane.getPixelStride()}).</p>\n     *\n     * <p>The U/V planes are guaranteed to have the same row stride and pixel stride\n     * (in particular,\n     * {@link android.media.Image.Plane#getRowStride() uPlane.getRowStride()}\n     * == {@link android.media.Image.Plane#getRowStride() vPlane.getRowStride()} and\n     * {@link android.media.Image.Plane#getPixelStride() uPlane.getPixelStride()}\n     * == {@link android.media.Image.Plane#getPixelStride() vPlane.getPixelStride()};\n     * ).</p>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.hardware.camera2.CameraDevice}\n     * through a {@link android.media.ImageReader} object.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.ImageReader\n     * @see android.hardware.camera2.CameraDevice\n     ",
    "links" : [ "android.media.Image.Plane#getRowStride()", "android.media.Image#getPlanes()", "android.media.Image", "android.media.Image.Plane#getPixelStride()", "android.hardware.camera2.CameraDevice", "android.media.ImageReader" ]
  }, {
    "name" : "YUV_422_888",
    "type" : "int",
    "comment" : "\n     * <p>Multi-plane Android YUV 422 format</p>\n     *\n     * <p>This format is a generic YCbCr format, capable of describing any 4:2:2\n     * chroma-subsampled (planar, semiplanar or interleaved) format,\n     * with 8 bits per color sample.</p>\n     *\n     * <p>Images in this format are always represented by three separate buffers\n     * of data, one for each color plane. Additional information always\n     * accompanies the buffers, describing the row stride and the pixel stride\n     * for each plane.</p>\n     *\n     * <p>The order of planes in the array returned by\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} is guaranteed such that\n     * plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V (Cr).</p>\n     *\n     * <p>In contrast to the {@link #YUV_420_888} format, the Y-plane may have a pixel\n     * stride greater than 1 in\n     * {@link android.media.Image.Plane#getPixelStride() yPlane.getPixelStride()}.</p>\n     *\n     * <p>The U/V planes are guaranteed to have the same row stride and pixel stride\n     * (in particular,\n     * {@link android.media.Image.Plane#getRowStride() uPlane.getRowStride()}\n     * == {@link android.media.Image.Plane#getRowStride() vPlane.getRowStride()} and\n     * {@link android.media.Image.Plane#getPixelStride() uPlane.getPixelStride()}\n     * == {@link android.media.Image.Plane#getPixelStride() vPlane.getPixelStride()};\n     * ).</p>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.media.MediaCodec}\n     * through {@link android.media.MediaCodec#getOutputImage} object.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.MediaCodec\n     ",
    "links" : [ "#YUV_420_888", "android.media.Image.Plane#getRowStride()", "android.media.MediaCodec", "android.media.Image#getPlanes()", "android.media.Image", "android.media.MediaCodec#getOutputImage", "android.media.Image.Plane#getPixelStride()" ]
  }, {
    "name" : "YUV_444_888",
    "type" : "int",
    "comment" : "\n     * <p>Multi-plane Android YUV 444 format</p>\n     *\n     * <p>This format is a generic YCbCr format, capable of describing any 4:4:4\n     * (planar, semiplanar or interleaved) format,\n     * with 8 bits per color sample.</p>\n     *\n     * <p>Images in this format are always represented by three separate buffers\n     * of data, one for each color plane. Additional information always\n     * accompanies the buffers, describing the row stride and the pixel stride\n     * for each plane.</p>\n     *\n     * <p>The order of planes in the array returned by\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} is guaranteed such that\n     * plane #0 is always Y, plane #1 is always U (Cb), and plane #2 is always V (Cr).</p>\n     *\n     * <p>In contrast to the {@link #YUV_420_888} format, the Y-plane may have a pixel\n     * stride greater than 1 in\n     * {@link android.media.Image.Plane#getPixelStride() yPlane.getPixelStride()}.</p>\n     *\n     * <p>The U/V planes are guaranteed to have the same row stride and pixel stride\n     * (in particular,\n     * {@link android.media.Image.Plane#getRowStride() uPlane.getRowStride()}\n     * == {@link android.media.Image.Plane#getRowStride() vPlane.getRowStride()} and\n     * {@link android.media.Image.Plane#getPixelStride() uPlane.getPixelStride()}\n     * == {@link android.media.Image.Plane#getPixelStride() vPlane.getPixelStride()};\n     * ).</p>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.media.MediaCodec}\n     * through {@link android.media.MediaCodec#getOutputImage} object.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.MediaCodec\n     ",
    "links" : [ "#YUV_420_888", "android.media.Image.Plane#getRowStride()", "android.media.MediaCodec", "android.media.Image#getPlanes()", "android.media.Image", "android.media.MediaCodec#getOutputImage", "android.media.Image.Plane#getPixelStride()" ]
  }, {
    "name" : "FLEX_RGB_888",
    "type" : "int",
    "comment" : "\n     * <p>Multi-plane Android RGB format</p>\n     *\n     * <p>This format is a generic RGB format, capable of describing most RGB formats,\n     * with 8 bits per color sample.</p>\n     *\n     * <p>Images in this format are always represented by three separate buffers\n     * of data, one for each color plane. Additional information always\n     * accompanies the buffers, describing the row stride and the pixel stride\n     * for each plane.</p>\n     *\n     * <p>The order of planes in the array returned by\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} is guaranteed such that\n     * plane #0 is always R (red), plane #1 is always G (green), and plane #2 is always B\n     * (blue).</p>\n     *\n     * <p>All three planes are guaranteed to have the same row strides and pixel strides.</p>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.media.MediaCodec}\n     * through {@link android.media.MediaCodec#getOutputImage} object.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.MediaCodec\n     ",
    "links" : [ "android.media.MediaCodec", "android.media.Image#getPlanes()", "android.media.Image", "android.media.MediaCodec#getOutputImage" ]
  }, {
    "name" : "FLEX_RGBA_8888",
    "type" : "int",
    "comment" : "\n     * <p>Multi-plane Android RGBA format</p>\n     *\n     * <p>This format is a generic RGBA format, capable of describing most RGBA formats,\n     * with 8 bits per color sample.</p>\n     *\n     * <p>Images in this format are always represented by four separate buffers\n     * of data, one for each color plane. Additional information always\n     * accompanies the buffers, describing the row stride and the pixel stride\n     * for each plane.</p>\n     *\n     * <p>The order of planes in the array returned by\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} is guaranteed such that\n     * plane #0 is always R (red), plane #1 is always G (green), plane #2 is always B (blue),\n     * and plane #3 is always A (alpha). This format may represent pre-multiplied or\n     * non-premultiplied alpha.</p>\n     *\n     * <p>All four planes are guaranteed to have the same row strides and pixel strides.</p>\n     *\n     * <p>For example, the {@link android.media.Image} object can provide data\n     * in this format from a {@link android.media.MediaCodec}\n     * through {@link android.media.MediaCodec#getOutputImage} object.</p>\n     *\n     * @see android.media.Image\n     * @see android.media.MediaCodec\n     ",
    "links" : [ "android.media.MediaCodec", "android.media.Image#getPlanes()", "android.media.Image", "android.media.MediaCodec#getOutputImage" ]
  }, {
    "name" : "RAW_SENSOR",
    "type" : "int",
    "comment" : "\n     * <p>General raw camera sensor image format, usually representing a\n     * single-channel Bayer-mosaic image. Each pixel color sample is stored with\n     * 16 bits of precision.</p>\n     *\n     * <p>The layout of the color mosaic, the maximum and minimum encoding\n     * values of the raw pixel data, the color space of the image, and all other\n     * needed information to interpret a raw sensor image must be queried from\n     * the {@link android.hardware.camera2.CameraDevice} which produced the\n     * image.</p>\n     ",
    "links" : [ "android.hardware.camera2.CameraDevice" ]
  }, {
    "name" : "RAW_PRIVATE",
    "type" : "int",
    "comment" : "\n     * <p>Private raw camera sensor image format, a single channel image with\n     * implementation depedent pixel layout.</p>\n     *\n     * <p>RAW_PRIVATE is a format for unprocessed raw image buffers coming from an\n     * image sensor. The actual structure of buffers of this format is\n     * implementation-dependent.</p>\n     *\n     ",
    "links" : [ ]
  }, {
    "name" : "RAW10",
    "type" : "int",
    "comment" : "\n     * <p>\n     * Android 10-bit raw format\n     * </p>\n     * <p>\n     * This is a single-plane, 10-bit per pixel, densely packed (in each row),\n     * unprocessed format, usually representing raw Bayer-pattern images coming\n     * from an image sensor.\n     * </p>\n     * <p>\n     * In an image buffer with this format, starting from the first pixel of\n     * each row, each 4 consecutive pixels are packed into 5 bytes (40 bits).\n     * Each one of the first 4 bytes contains the top 8 bits of each pixel, The\n     * fifth byte contains the 2 least significant bits of the 4 pixels, the\n     * exact layout data for each 4 consecutive pixels is illustrated below\n     * ({@code Pi[j]} stands for the jth bit of the ith pixel):\n     * </p>\n     * <table>\n     * <thead>\n     * <tr>\n     * <th align=\"center\"></th>\n     * <th align=\"center\">bit 7</th>\n     * <th align=\"center\">bit 6</th>\n     * <th align=\"center\">bit 5</th>\n     * <th align=\"center\">bit 4</th>\n     * <th align=\"center\">bit 3</th>\n     * <th align=\"center\">bit 2</th>\n     * <th align=\"center\">bit 1</th>\n     * <th align=\"center\">bit 0</th>\n     * </tr>\n     * </thead> <tbody>\n     * <tr>\n     * <td align=\"center\">Byte 0:</td>\n     * <td align=\"center\">P0[9]</td>\n     * <td align=\"center\">P0[8]</td>\n     * <td align=\"center\">P0[7]</td>\n     * <td align=\"center\">P0[6]</td>\n     * <td align=\"center\">P0[5]</td>\n     * <td align=\"center\">P0[4]</td>\n     * <td align=\"center\">P0[3]</td>\n     * <td align=\"center\">P0[2]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 1:</td>\n     * <td align=\"center\">P1[9]</td>\n     * <td align=\"center\">P1[8]</td>\n     * <td align=\"center\">P1[7]</td>\n     * <td align=\"center\">P1[6]</td>\n     * <td align=\"center\">P1[5]</td>\n     * <td align=\"center\">P1[4]</td>\n     * <td align=\"center\">P1[3]</td>\n     * <td align=\"center\">P1[2]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 2:</td>\n     * <td align=\"center\">P2[9]</td>\n     * <td align=\"center\">P2[8]</td>\n     * <td align=\"center\">P2[7]</td>\n     * <td align=\"center\">P2[6]</td>\n     * <td align=\"center\">P2[5]</td>\n     * <td align=\"center\">P2[4]</td>\n     * <td align=\"center\">P2[3]</td>\n     * <td align=\"center\">P2[2]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 3:</td>\n     * <td align=\"center\">P3[9]</td>\n     * <td align=\"center\">P3[8]</td>\n     * <td align=\"center\">P3[7]</td>\n     * <td align=\"center\">P3[6]</td>\n     * <td align=\"center\">P3[5]</td>\n     * <td align=\"center\">P3[4]</td>\n     * <td align=\"center\">P3[3]</td>\n     * <td align=\"center\">P3[2]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 4:</td>\n     * <td align=\"center\">P3[1]</td>\n     * <td align=\"center\">P3[0]</td>\n     * <td align=\"center\">P2[1]</td>\n     * <td align=\"center\">P2[0]</td>\n     * <td align=\"center\">P1[1]</td>\n     * <td align=\"center\">P1[0]</td>\n     * <td align=\"center\">P0[1]</td>\n     * <td align=\"center\">P0[0]</td>\n     * </tr>\n     * </tbody>\n     * </table>\n     * <p>\n     * This format assumes\n     * <ul>\n     * <li>a width multiple of 4 pixels</li>\n     * <li>an even height</li>\n     * </ul>\n     * </p>\n     *\n     * <pre>size = row stride * height</pre> where the row stride is in <em>bytes</em>,\n     * not pixels.\n     *\n     * <p>\n     * Since this is a densely packed format, the pixel stride is always 0. The\n     * application must use the pixel data layout defined in above table to\n     * access each row data. When row stride is equal to {@code width * (10 / 8)}, there\n     * will be no padding bytes at the end of each row, the entire image data is\n     * densely packed. When stride is larger than {@code width * (10 / 8)}, padding\n     * bytes will be present at the end of each row.\n     * </p>\n     * <p>\n     * For example, the {@link android.media.Image} object can provide data in\n     * this format from a {@link android.hardware.camera2.CameraDevice} (if\n     * supported) through a {@link android.media.ImageReader} object. The\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} will return a\n     * single plane containing the pixel data. The pixel stride is always 0 in\n     * {@link android.media.Image.Plane#getPixelStride()}, and the\n     * {@link android.media.Image.Plane#getRowStride()} describes the vertical\n     * neighboring pixel distance (in bytes) between adjacent rows.\n     * </p>\n     *\n     * @see android.media.Image\n     * @see android.media.ImageReader\n     * @see android.hardware.camera2.CameraDevice\n     ",
    "links" : [ "android.media.Image.Plane#getRowStride()", "android.media.Image#getPlanes()", "android.media.Image", "android.media.Image.Plane#getPixelStride()", "android.hardware.camera2.CameraDevice", "android.media.ImageReader" ]
  }, {
    "name" : "RAW12",
    "type" : "int",
    "comment" : "\n     * <p>\n     * Android 12-bit raw format\n     * </p>\n     * <p>\n     * This is a single-plane, 12-bit per pixel, densely packed (in each row),\n     * unprocessed format, usually representing raw Bayer-pattern images coming\n     * from an image sensor.\n     * </p>\n     * <p>\n     * In an image buffer with this format, starting from the first pixel of each\n     * row, each two consecutive pixels are packed into 3 bytes (24 bits). The first\n     * and second byte contains the top 8 bits of first and second pixel. The third\n     * byte contains the 4 least significant bits of the two pixels, the exact layout\n     * data for each two consecutive pixels is illustrated below (Pi[j] stands for\n     * the jth bit of the ith pixel):\n     * </p>\n     * <table>\n     * <thead>\n     * <tr>\n     * <th align=\"center\"></th>\n     * <th align=\"center\">bit 7</th>\n     * <th align=\"center\">bit 6</th>\n     * <th align=\"center\">bit 5</th>\n     * <th align=\"center\">bit 4</th>\n     * <th align=\"center\">bit 3</th>\n     * <th align=\"center\">bit 2</th>\n     * <th align=\"center\">bit 1</th>\n     * <th align=\"center\">bit 0</th>\n     * </tr>\n     * </thead> <tbody>\n     * <tr>\n     * <td align=\"center\">Byte 0:</td>\n     * <td align=\"center\">P0[11]</td>\n     * <td align=\"center\">P0[10]</td>\n     * <td align=\"center\">P0[ 9]</td>\n     * <td align=\"center\">P0[ 8]</td>\n     * <td align=\"center\">P0[ 7]</td>\n     * <td align=\"center\">P0[ 6]</td>\n     * <td align=\"center\">P0[ 5]</td>\n     * <td align=\"center\">P0[ 4]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 1:</td>\n     * <td align=\"center\">P1[11]</td>\n     * <td align=\"center\">P1[10]</td>\n     * <td align=\"center\">P1[ 9]</td>\n     * <td align=\"center\">P1[ 8]</td>\n     * <td align=\"center\">P1[ 7]</td>\n     * <td align=\"center\">P1[ 6]</td>\n     * <td align=\"center\">P1[ 5]</td>\n     * <td align=\"center\">P1[ 4]</td>\n     * </tr>\n     * <tr>\n     * <td align=\"center\">Byte 2:</td>\n     * <td align=\"center\">P1[ 3]</td>\n     * <td align=\"center\">P1[ 2]</td>\n     * <td align=\"center\">P1[ 1]</td>\n     * <td align=\"center\">P1[ 0]</td>\n     * <td align=\"center\">P0[ 3]</td>\n     * <td align=\"center\">P0[ 2]</td>\n     * <td align=\"center\">P0[ 1]</td>\n     * <td align=\"center\">P0[ 0]</td>\n     * </tr>\n     * </tbody>\n     * </table>\n     * <p>\n     * This format assumes\n     * <ul>\n     * <li>a width multiple of 4 pixels</li>\n     * <li>an even height</li>\n     * </ul>\n     * </p>\n     *\n     * <pre>size = row stride * height</pre> where the row stride is in <em>bytes</em>,\n     * not pixels.\n     *\n     * <p>\n     * Since this is a densely packed format, the pixel stride is always 0. The\n     * application must use the pixel data layout defined in above table to\n     * access each row data. When row stride is equal to {@code width * (12 / 8)}, there\n     * will be no padding bytes at the end of each row, the entire image data is\n     * densely packed. When stride is larger than {@code width * (12 / 8)}, padding\n     * bytes will be present at the end of each row.\n     * </p>\n     * <p>\n     * For example, the {@link android.media.Image} object can provide data in\n     * this format from a {@link android.hardware.camera2.CameraDevice} (if\n     * supported) through a {@link android.media.ImageReader} object. The\n     * {@link android.media.Image#getPlanes() Image#getPlanes()} will return a\n     * single plane containing the pixel data. The pixel stride is always 0 in\n     * {@link android.media.Image.Plane#getPixelStride()}, and the\n     * {@link android.media.Image.Plane#getRowStride()} describes the vertical\n     * neighboring pixel distance (in bytes) between adjacent rows.\n     * </p>\n     *\n     * @see android.media.Image\n     * @see android.media.ImageReader\n     * @see android.hardware.camera2.CameraDevice\n     ",
    "links" : [ "android.media.Image.Plane#getRowStride()", "android.media.Image#getPlanes()", "android.media.Image", "android.media.Image.Plane#getPixelStride()", "android.hardware.camera2.CameraDevice", "android.media.ImageReader" ]
  }, {
    "name" : "DEPTH16",
    "type" : "int",
    "comment" : "\n     * <p>Android dense depth image format.</p>\n     *\n     * <p>Each pixel is 16 bits, representing a depth ranging measurement from a depth camera or\n     * similar sensor. The 16-bit sample consists of a confidence value and the actual ranging\n     * measurement.</p>\n     *\n     * <p>The confidence value is an estimate of correctness for this sample.  It is encoded in the\n     * 3 most significant bits of the sample, with a value of 0 representing 100% confidence, a\n     * value of 1 representing 0% confidence, a value of 2 representing 1/7, a value of 3\n     * representing 2/7, and so on.</p>\n     *\n     * <p>As an example, the following sample extracts the range and confidence from the first pixel\n     * of a DEPTH16-format {@link android.media.Image}, and converts the confidence to a\n     * floating-point value between 0 and 1.f inclusive, with 1.f representing maximum confidence:\n     *\n     * <pre>\n     *    ShortBuffer shortDepthBuffer = img.getPlanes()[0].getBuffer().asShortBuffer();\n     *    short depthSample = shortDepthBuffer.get()\n     *    short depthRange = (short) (depthSample & 0x1FFF);\n     *    short depthConfidence = (short) ((depthSample >> 13) & 0x7);\n     *    float depthPercentage = depthConfidence == 0 ? 1.f : (depthConfidence - 1) / 7.f;\n     * </pre>\n     * </p>\n     *\n     * <p>This format assumes\n     * <ul>\n     * <li>an even width</li>\n     * <li>an even height</li>\n     * <li>a horizontal stride multiple of 16 pixels</li>\n     * </ul>\n     * </p>\n     *\n     * <pre> y_size = stride * height </pre>\n     *\n     * When produced by a camera, the units for the range are millimeters.\n     ",
    "links" : [ "android.media.Image" ]
  }, {
    "name" : "DEPTH_POINT_CLOUD",
    "type" : "int",
    "comment" : "\n     * Android sparse depth point cloud format.\n     *\n     * <p>A variable-length list of 3D points plus a confidence value, with each point represented\n     * by four floats; first the X, Y, Z position coordinates, and then the confidence value.</p>\n     *\n     * <p>The number of points is {@code (size of the buffer in bytes) / 16}.\n     *\n     * <p>The coordinate system and units of the position values depend on the source of the point\n     * cloud data. The confidence value is between 0.f and 1.f, inclusive, with 0 representing 0%\n     * confidence and 1.f representing 100% confidence in the measured position values.</p>\n     *\n     * <p>As an example, the following code extracts the first depth point in a DEPTH_POINT_CLOUD\n     * format {@link android.media.Image}:\n     * <pre>\n     *    FloatBuffer floatDepthBuffer = img.getPlanes()[0].getBuffer().asFloatBuffer();\n     *    float x = floatDepthBuffer.get();\n     *    float y = floatDepthBuffer.get();\n     *    float z = floatDepthBuffer.get();\n     *    float confidence = floatDepthBuffer.get();\n     * </pre>\n     *\n     * For camera devices that support the\n     * {@link android.hardware.camera2.CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT DEPTH_OUTPUT}\n     * capability, DEPTH_POINT_CLOUD coordinates have units of meters, and the coordinate system is\n     * defined by the camera's pose transforms:\n     * {@link android.hardware.camera2.CameraCharacteristics#LENS_POSE_TRANSLATION} and\n     * {@link android.hardware.camera2.CameraCharacteristics#LENS_POSE_ROTATION}. That means the origin is\n     * the optical center of the camera device, and the positive Z axis points along the camera's optical axis,\n     * toward the scene.\n     ",
    "links" : [ "android.media.Image", "android.hardware.camera2.CameraCharacteristics#LENS_POSE_ROTATION", "android.hardware.camera2.CameraCharacteristics#LENS_POSE_TRANSLATION", "android.hardware.camera2.CameraCharacteristics#REQUEST_AVAILABLE_CAPABILITIES_DEPTH_OUTPUT" ]
  }, {
    "name" : "RAW_DEPTH",
    "type" : "int",
    "comment" : "\n     * Unprocessed implementation-dependent raw\n     * depth measurements, opaque with 16 bit\n     * samples.\n     *\n     * @hide\n     ",
    "links" : [ ]
  }, {
    "name" : "PRIVATE",
    "type" : "int",
    "comment" : "\n     * Android private opaque image format.\n     * <p>\n     * The choices of the actual format and pixel data layout are entirely up to\n     * the device-specific and framework internal implementations, and may vary\n     * depending on use cases even for the same device. The buffers of this\n     * format can be produced by components like\n     * {@link android.media.ImageWriter ImageWriter} , and interpreted correctly\n     * by consumers like {@link android.hardware.camera2.CameraDevice\n     * CameraDevice} based on the device/framework private information. However,\n     * these buffers are not directly accessible to the application.\n     * </p>\n     * <p>\n     * When an {@link android.media.Image Image} of this format is obtained from\n     * an {@link android.media.ImageReader ImageReader} or\n     * {@link android.media.ImageWriter ImageWriter}, the\n     * {@link android.media.Image#getPlanes() getPlanes()} method will return an\n     * empty {@link android.media.Image.Plane Plane} array.\n     * </p>\n     * <p>\n     * If a buffer of this format is to be used as an OpenGL ES texture, the\n     * framework will assume that sampling the texture will always return an\n     * alpha value of 1.0 (i.e. the buffer contains only opaque pixel values).\n     * </p>\n     ",
    "links" : [ "android.media.Image#getPlanes()", "android.hardware.camera2.CameraDeviceCameraDevice", "android.media.Image", "android.media.ImageWriter", "android.media.ImageReader", "android.media.Image.Plane" ]
  }, {
    "name" : "HEIC",
    "type" : "int",
    "comment" : "\n     * Compressed HEIC format.\n     *\n     * <p>This format defines the HEIC brand of High Efficiency Image File\n     * Format as described in ISO/IEC 23008-12.</p>\n     ",
    "links" : [ ]
  } ],
  "methods" : [ {
    "name" : "public static int getBitsPerPixel(@Format int format)",
    "returnType" : "int",
    "comment" : "\n     * Use this function to retrieve the number of bits per pixel of an\n     * ImageFormat.\n     *\n     * @param format\n     * @return the number of bits per pixel of the given format or -1 if the\n     *         format doesn't exist or is not supported.\n     ",
    "links" : [ ]
  }, {
    "name" : "public static boolean isPublicFormat(@Format int format)",
    "returnType" : "boolean",
    "comment" : "\n     * Determine whether or not this is a public-visible {@code format}.\n     *\n     * <p>In particular, {@code @hide} formats will return {@code false}.</p>\n     *\n     * <p>Any other formats (including UNKNOWN) will return {@code false}.</p>\n     *\n     * @param format an integer format\n     * @return a boolean\n     *\n     * @hide\n     ",
    "links" : [ ]
  } ],
  "variableNames" : [ "UNKNOWN", "RGB_565", "YV12", "Y8", "Y16", "NV16", "NV21", "YUY2", "JPEG", "DEPTH_JPEG", "YUV_420_888", "YUV_422_888", "YUV_444_888", "FLEX_RGB_888", "FLEX_RGBA_8888", "RAW_SENSOR", "RAW_PRIVATE", "RAW10", "RAW12", "DEPTH16", "DEPTH_POINT_CLOUD", "RAW_DEPTH", "PRIVATE", "HEIC" ],
  "methodNames" : [ "public static int getBitsPerPixel(@Format int format)", "public static boolean isPublicFormat(@Format int format)" ]
}