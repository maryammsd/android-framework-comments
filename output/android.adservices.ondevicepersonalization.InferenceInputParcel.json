{
  "filePath" : "/home/maryam/clearblue/files/android-source-35/android/adservices/ondevicepersonalization/InferenceInputParcel.java",
  "packageName" : "android.adservices.ondevicepersonalization",
  "className" : "InferenceInputParcel",
  "comment" : "\n * Parcelable version of {@link InferenceInput}.\n *\n * @hide\n ",
  "links" : [ "android.adservices.ondevicepersonalization.InferenceInput" ],
  "variables" : [ {
    "name" : "mModelId",
    "type" : "ModelId",
    "comment" : "\n     * The location of TFLite model. The model is usually store in REMOTE_DATA or LOCAL_DATA table.\n     ",
    "links" : [ ]
  }, {
    "name" : "mDelegate",
    "type" : "int",
    "comment" : " The delegate to run model inference. If not specified, CPU delegate is used by default. ",
    "links" : [ ]
  }, {
    "name" : "mCpuNumThread",
    "type" : "int",
    "comment" : "\n     * The number of threads available to the interpreter. Only set and take effective when input\n     * tensors are on CPU. Setting cpuNumThread to 0 has the effect to disable multithreading, which\n     * is equivalent to setting cpuNumThread to 1. If set to the value -1, the number of threads\n     * used will be implementation-defined and platform-dependent.\n     ",
    "links" : [ ]
  }, {
    "name" : "mInputData",
    "type" : "ByteArrayParceledListSlice",
    "comment" : " An array of input data. The inputs should be in the same order as inputs of the model. ",
    "links" : [ ]
  }, {
    "name" : "mBatchSize",
    "type" : "int",
    "comment" : "\n     * The number of input examples. Adopter can set this field to run batching inference. The batch\n     * size is 1 by default.\n     ",
    "links" : [ ]
  }, {
    "name" : "mModelType",
    "type" : "int",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "mExpectedOutputStructure",
    "type" : "InferenceOutputParcel",
    "comment" : "\n     * The empty InferenceOutput representing the expected output structure. For TFLite, the\n     * inference code will verify whether this expected output structure matches model output\n     * signature.\n     ",
    "links" : [ ]
  }, {
    "name" : "CREATOR",
    "type" : "Parcelable.Creator<InferenceInputParcel>",
    "comment" : "",
    "links" : [ ]
  } ],
  "methods" : [ {
    "name" : "public ModelId getModelId()",
    "returnType" : "ModelId",
    "comment" : "\n     * The location of TFLite model. The model is usually store in REMOTE_DATA or LOCAL_DATA table.\n     ",
    "links" : [ ]
  }, {
    "name" : "public int getDelegate()",
    "returnType" : "int",
    "comment" : " The delegate to run model inference. If not specified, CPU delegate is used by default. ",
    "links" : [ ]
  }, {
    "name" : "public int getCpuNumThread()",
    "returnType" : "int",
    "comment" : "\n     * The number of threads available to the interpreter. Only set and take effective when input\n     * tensors are on CPU. Setting cpuNumThread to 0 has the effect to disable multithreading, which\n     * is equivalent to setting cpuNumThread to 1. If set to the value -1, the number of threads\n     * used will be implementation-defined and platform-dependent.\n     ",
    "links" : [ ]
  }, {
    "name" : "public ByteArrayParceledListSlice getInputData()",
    "returnType" : "ByteArrayParceledListSlice",
    "comment" : " An array of input data. The inputs should be in the same order as inputs of the model. ",
    "links" : [ ]
  }, {
    "name" : "public int getBatchSize()",
    "returnType" : "int",
    "comment" : "\n     * The number of input examples. Adopter can set this field to run batching inference. The batch\n     * size is 1 by default.\n     ",
    "links" : [ ]
  }, {
    "name" : "public int getModelType()",
    "returnType" : "int",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "public InferenceOutputParcel getExpectedOutputStructure()",
    "returnType" : "InferenceOutputParcel",
    "comment" : "\n     * The empty InferenceOutput representing the expected output structure. For TFLite, the\n     * inference code will verify whether this expected output structure matches model output\n     * signature.\n     ",
    "links" : [ ]
  }, {
    "name" : "public void writeToParcel(@NonNull android.os.Parcel dest, int flags)",
    "returnType" : "void",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "public int describeContents()",
    "returnType" : "int",
    "comment" : "",
    "links" : [ ]
  }, {
    "name" : "private void __metadata()",
    "returnType" : "void",
    "comment" : "",
    "links" : [ ]
  } ],
  "methodNames" : [ "public ModelId getModelId()", "public int getDelegate()", "public int getCpuNumThread()", "public ByteArrayParceledListSlice getInputData()", "public int getBatchSize()", "public int getModelType()", "public InferenceOutputParcel getExpectedOutputStructure()", "public void writeToParcel(@NonNull android.os.Parcel dest, int flags)", "public int describeContents()", "private void __metadata()" ],
  "variableNames" : [ "mModelId", "mDelegate", "mCpuNumThread", "mInputData", "mBatchSize", "mModelType", "mExpectedOutputStructure", "CREATOR" ]
}